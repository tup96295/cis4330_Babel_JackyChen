# cis4330_Babel_JackyChen
A Scalable Pre-trained Model for Multi-Modal Sensing via Expandable Modality Alignment
Sensors that will be used in the project:
 - Camera – to capture images or video of the environment
 - Microphone – to capture surrounding sounds
 - Accelerometer – to detect movement and motion patterns
 - Gyroscope – to detect device orientation and rotation
 - GPS – to detect location and movement speed


Goal: The goal of this project is to create a scaleable, multi-model sensing application that gathers and combines all the data from the sensors to allow for the ability to adapt to the user’s action and environment. Additionally, this application would have the ability to retain information even if a new sensor is added.

Scenario: 
 1.The user goes through his daily routine normally, and while he does all that, the sensor would be gathering data as to what is happening. Over time, the data collected would then be used by the sensors to create a routine to reduce excess activity. An instance can be the gyroscope, used to detect device orientation, is turned off based on the routine or pattern from the data that the sensor gathered. 
 2.Another scenario would be the difference between the user walking/comuting or just lounging. By understanding the routine of when you are commuting and when you might be lounging around, they could control “when might be the best time to send you a notification”. Finally and most importantly, if there is ever a hardware update that adds a new sensor, the pre-existing sensors won’t be affected at all.
